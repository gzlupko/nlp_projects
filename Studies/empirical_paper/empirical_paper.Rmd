---
title: "Gian Zlupko Empirical Paper"
output: html_notebook
---

# Libraries and Setup 
Data analysis related to CDC compliance behaviors (2021) 

```{r Libraries-and-Path, include = F}

library(tidytext) 
library(tidyverse)
library(topicmodels) 
library(ldatuning)
library(textstem) 
library(SnowballC) 
library(tm)
library(car) 
library(stm) 
library(wesanderson)

```

# Data Cleaning 

```{r Data-Cleaning}
# load raw data and begin conversion to long data format 
# this row for reasons only data (no state or vax covariates)
#dat <- read_csv("cdc_raw_reasons.csv")

# this row for covariates included in broader data set: state and vax
dat <- read_csv("cdc_full_data.csv")

# rename columns

dat <- dat %>%
   rename(Decision = Q4, Reason1 = Q12_1, Reason2 = Q12_2, Reason3 = Q12_3, 
          Reason4 = Q12_4, Reason5 = Q12_5, Reason6 = Q12_6, Reason7 = Q12_7, 
          Reason8 = Q12_8, Reason9 = Q12_9, Reason10 = Q12_10)  

# rename additional columns to shorter headers 
dat <- dat %>%
  rename(MacroReasoning = MacroReasoning_3_items_standardized_Ave)


# first add unique participant id 
dat1 <- dat %>%
  mutate(participant_id = 1:length(Decision))

# use duplicated() to check that each participant_id is unique 
duplicated(dat1[ ,c("participant_id")])

# after confirming that all ids are unique
# create one data set of reasons that will be 
# used to convert from wide to long format 

dat2 <- dat1 %>%
  select(participant_id, Reason1, Reason2, Reason3, Reason3, 
         Reason4, Reason5, Reason6, Reason7, Reason8, Reason9, Reason10)  

# first convert reasons to long data format
data_long <- dat2 %>%
  gather(key = "Reason", 
         value = "Reason_Stated", c(-participant_id)) 


# next create a subset of the original data without the reasons data
# instead, retain only the BRT and decision variable scores to reattach
# to the long data frame

scores_to_merge <- dat1 %>%
  select(participant_id, ConfRfor, ConfRag, Att_Ave, SN_Ave, PC_Ave, MacroReasoning, 
         SN_general_Ave, SN_work_Ave, ProReas, ConReas, State)

# now, merge with the long formatted reasons data and merge by participant_id

reasons_formatted <- merge(x = data_long, 
                           y = scores_to_merge, 
                           by = "participant_id", 
                           all.y = T)


# finally remove rows with NA values 

reasons_long <- reasons_formatted %>%
  filter(!is.na(Reason_Stated)) 


# add row_id, which will be used after creating topic model
# to re-assign the topic model output back to the unique row id 
reasons_long$row_id <- paste(1:nrow(reasons_long))


```


# Topic Modeling


## Text Pre-processing 

Structural Topic Modeling Framework
Resources: 
1. Roberts et al (2018) - STM: R Package for Structural Topic Models: 
https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf

2. University of Oregon: 
https://blogs.uoregon.edu/rclub/2016/04/05/structural-topic-modeling/

```{r}
reasons_long <- reasons_long %>%
  rename(document = Reason_Stated) 

# test remove NA from ConfRfor
#reasons_long <- reasons_long %>%
  #filter(!is.na(ConfRfor)) 

# uses textProcessor() function from tm package 
processed <- textProcessor(reasons_long$document, metadata = reasons_long)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)

# run an initial 4-topic fit to explore initial elements in the corpus 
fit0 <- stm(out$documents, # the documents
            out$vocab, # the words
            K = 4, # 10 topics
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral")  

# look at top words
labelTopics(fit0)

plot.STM(fit0, type = "summary") 

# correlations between topics 
round(topicCorr(fit0)$cor, 2) # just the correlations between topics


```


## Evaluate Model Fit 

Model Diagnostics: examining semantic coherence and exclusivity 

```{r, include F}
# identify best k out of 10 topics 
ntopics <- searchK(out$documents, out$vocab, K = c(2, 3 , 4, 5, 6, 7, 8, 9, 10), data = meta)

# fit statistics for k = c(2:10) estimations  
plot(ntopics) 

# selectModel() approach can also be used in addition to searchK() 
# whereby models are selected that optimize on exclusivity and semantic coherence 

stm_select_mod <- selectModel(out$documents, out$vocab, K = 15, 
                              prevalence = ~State, max.em.its = 75, data = out$meta, 
                              runs = 15) 


plotModels(stm_select_mod, pch = c(1,2,3,4), legend.position = "bottomright")
```


## Topics: Reasons for decision 

Reasons For data - Topic Model Comparisons

The following code chunks compare 2, 3, and 4-topic model solutions to select the best one. First, a 2-topic model solution is generated along with corresponding figures, tables, and estimated effect analyses. 
```{r stm-2-topic}

fit_two <- stm(out$documents, # the documents
            out$vocab, # covariates expected to affect topic content 
            prevalence =~ participant_id + State,
            K = 2, # 2 topics
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral") 

sageLabels(fit_two) # only works with content covariates in the model
plot(fit_two)

# correlations between topics 
round(topicCorr(fit0)$cor, 2) # just the correlations between topics
```



Bar chart for 2-topic model solution fit 

```{r}
# Turn the STM object into a data frame. This is necessary so that we can work with it.
td_beta <- tidy(fit_two)

# create re-useable function for subsequent topic model solutions 
# bar chart uses wesanderson color palettes
word_probs <- function(td_beta) { 
  
  td_beta %>%
  # Group by topic
  group_by(topic) %>%
  # Take the top 10 based on beta
  top_n(10, beta) %>%
  # Ungroup
  ungroup() %>%
  # Generate the variables topic and term
  dplyr::mutate(topic = paste0(topic),
                term = reorder_within(term, beta, topic)) %>%
  # And plot it
  ggplot() +
  # Using a bar plot with the terms on the x-axis, the beta on the y-axis, filled by topic
  geom_col(aes(x = term, y = beta, fill = as.factor(topic)),
           alpha = 0.8,
           show.legend = FALSE) +
  # Do a facet_wrap by topic
  facet_wrap(~ topic, scales = "free_y") +
  # And flip the plot
  coord_flip() +
  scale_x_reordered() +
  # Label the x-axis, y-axis, as well as title
  labs(
    y = expression(beta),
    title = "Highest word probabilities for each topic") +
  # And finally define the colors
  scale_fill_manual(values = wes_palette("Darjeeling1"))
  
  
  }

# generate bar chart for 2 topic solution
word_probs(td_beta) 

```



```{r}
# look at highest word probability
labelTopics(fit_two)

plot.STM(fit_two, type = "labels") 

plot.STM(fit_two, type = "summary", main = "2-Topic Expected Proportions") 
# look at 
plot.STM(fit_two, type = "labels", labeltype = c("frex"), main = "2-Topic Solution: Exclusivity")  

# outputs most representative documents for a particular topic
findThoughts(
  # Your topic model
  fit_two,
  texts = reasons_long, 
  n = 3, 
  topics = c(1,2,3)
)
```


Use model outputs to assign categorical variables for regression 

```{r}

# Attitude 
est_two_1 <- estimateEffect(1:2 ~ Att_Ave, stmobj = fit_two, metadata = out$meta, 
                       uncertainty = "None")   

summary(est_two_1) 

# Subjective Norm 

est_two_2 <- estimateEffect(1:2 ~ SN_Ave, stmobj = fit_two, metadata = out$meta, 
                       uncertainty = "None")   

summary(est_two_2)

# Perceived Control

est_two_3 <- estimateEffect(1:2 ~ PC_Ave, stmobj = fit_two, metadata = out$meta, 
                       uncertainty = "None")   

summary(est_two_3)

plot(est_two_1, covariate = "Att_Ave", topics = c(1,2), 
     model = fit_two, 
     method = "continuous") 

# show the estimated relationship between Topic 1 and Attitude 
plot(est_two_1, covariate = "Att_Ave", model = fit_two,
     method = "continuous", xlab = "Attitude", topics = c(1))
```


Generate a 3-topic solution with figures and diagnostics. 

```{r stm-3-topic}

fit_three <- stm(out$documents, # the documents
            out$vocab, # covariates expected to affect topic content 
            prevalence =~ participant_id + State,
            K = 3, # 2 topics
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral", 
            seed = 10901) 

sageLabels(fit_three) # only works with content covariates in the model
plot(fit_three)

# correlations between topics 
round(topicCorr(fit_three)$cor, 2) #correlations between topics

# highest word probabilities bar chart 
td_beta <- tidy(fit_three) 
word_probs(td_beta)

# look at highest word probability
labelTopics(fit_three)

# top terms conditioning on exclusivity of terms by between topics 
plot.STM(fit_three, type = "labels", labeltype = c("frex"), main = "3-Topic Solution: Exclusivity")  

# perpsective plot 
plot.STM(fit_three, type = "perspectives", topics = c(2,3))   

# expected proportion of topics in all reasons for data 
plot.STM(fit_three, type = "summary", main = "3-Topic Expected Proportions") 


# custom approach to summarizing topic proportions 

reasons_for_proportions <- make.dt(fit_three) 

summarize_all(reasons_for_proportions, mean)


# write topic betas to a csv file for manual graphics 

td_beta <- tidy(fit_three) 
td_beta <- td_beta %>%
   mutate(topic = ifelse(topic == 1, "To Protect Others",  
                         ifelse(topic == 2, "To Protect Self", 
                                ifelse(topic == 3, "To Avoid Spread", NA)))) %>%
  mutate(topic = as.factor(topic)) %>%
  group_by(topic) %>% arrange(desc(beta)) 


# update select character strings to undo stemming for publication 

td_beta$term <- str_replace_all(td_beta$term, "healthi", "health")
td_beta$term <- str_replace_all(td_beta$term, "safeti", "safety")
td_beta$term <- str_replace_all(td_beta$term, "famili", "family")
td_beta$term <- str_replace_all(td_beta$term, "peopl", "people")
td_beta$term <- str_replace_all(td_beta$term, "exampl", "example")
td_beta$term <- str_replace_all(td_beta$term, "practic", "practices")
word_probs(td_beta) 

```
Calculate estimated effects for the 3-topic solution

```{r stm-3-topic-estimates}

# Attitude 
est_three_1 <- estimateEffect(1:3 ~ Att_Ave, stmobj = fit_three, metadata = out$meta, 
                              uncertainty = "None")    

summary(est_three_1) 

# Subjective Norm 

est_three_2 <- estimateEffect(1:3 ~ SN_Ave, stmobj = fit_three, metadata = out$meta, 
                       uncertainty = "None")   

summary(est_three_2)

# Perceived Control

est_three_3 <- estimateEffect(1:3 ~ PC_Ave, stmobj = fit_three, metadata = out$meta, 
                       uncertainty = "None")   

summary(est_three_3)



plot(est_two_1, covariate = "Att_Ave", topics = c(1,2), 
     model = fit_two, 
     method = "continuous") 

# show the estimated relationship between Topic 3 and PC 
plot(est_three_3, covariate = "PC_Ave", model = fit_three,
     method = "continuous", xlab = "Perceived Control", topics = c(3))


```


Generate a 4-topic model solution for comparison


```{r stm-4-topic-model}

fit_four <- stm(out$documents, # the documents
            out$vocab, # covariates expected to affect topic content 
            prevalence =~ participant_id + State,
            K = 4, # 2 topics
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral") 

sageLabels(fit_four) # only works with content covariates in the model
plot(fit_four)

# correlations between topics 
round(topicCorr(fit_four)$cor, 2) # shows that correlations


# highest word probabilities bar chart 
td_beta <- tidy(fit_four) 
word_probs(td_beta)

# look at highest word probability
labelTopics(fit_four)

# top terms conditioning on exclusivity of terms by between topics 
plot.STM(fit_four, type = "labels", labeltype = c("frex"), main = "4-Topic Solution: Exclusivity")  

# perpsective plot: can add additional context on differentiation b/w two specific topics in the solution
plot.STM(fit_four, type = "perspectives", topics = c(3,4))   

# expected proportion of topics in all reasons for data 
plot.STM(fit_four, type = "summary", main = "4-Topic Expected Proportion")  

```



Calculate estimated effects for 4-topic solution 

```{r stm-4-topic-estimates}

# Attitude 
est_four_1 <- estimateEffect(1:4 ~ Att_Ave, stmobj = fit_four, metadata = out$meta, 
                             uncertainty = "None")   

summary(est_four_1) 

# Subjective Norm 

est_four_2 <- estimateEffect(1:4 ~ SN_Ave, stmobj = fit_four, metadata = out$meta, 
                       uncertainty = "None")   
summary(est_four_2)

# Perceived Control

est_four_3 <- estimateEffect(1:4 ~ PC_Ave, stmobj = fit_four, metadata = out$meta, 
                       uncertainty = "None")   
summary(est_four_3)

```




```{r stm-fit-statistics}
set.seed(6110) 
# use searchK() function to calculate fit statistics for 2, 3, and 4 topic solutions 
ntopics <- searchK(out$documents, out$vocab, K = c(2, 3, 4), data = meta)

# grab fit statistic values with the results df 
ntopics$results

# plot ntopics for fit statistic plots 
plot(ntopics) 

```



## Topics: Reasons Against decision

Reasons Against Data

Reasons Against data - Topic Model Comparisons

The following code chunks compare 2, 3, and 4-topic model solutions to select the best model for the reasons against data. First, a 2-topic model solution is generated along with corresponding figures, tables, and estimated effect analyses. 


```{r Against-Data-Cleaning}
# rename columns

against <- dat %>%
   rename(Against1 = Q33_1, Against2 = Q33_2, Against3 = Q33_3, 
          Against4 = Q33_4, Against5 = Q33_5, Against6 = Q33_6, Against7 = Q33_7, 
          Against8 = Q33_8, Against9 = Q33_9, Against10 = Q33_10)  


# first add unique participant id 
against1 <- against %>%
  mutate(participant_id = 1:length(Decision))

# use duplicated() to check that each participant_id is unique 
duplicated(against1[ ,c("participant_id")])

# after confirming that all ids are unique
# create one data set of reasons that will be 
# used to convert from wide to long format 

against_df <- against1 %>%
  select(participant_id, Against1, Against2, Against3, Against4, 
         Against5, Against6, Against7, Against8, Against9, Against10)  

# first convert reasons to long data format
against_long <- against_df %>%
  gather(key = "Against", 
         value = "Against_Stated", c(-participant_id)) 


# next create a subset of the original data without the reasons data
# instead, retain only the BRT and decision variable scores to reattach
# to the long data frame

against_to_merge <- against1 %>%
  select(participant_id, ConfRfor, ConfRag, Att_Ave, SN_Ave, PC_Ave, MacroReasoning, 
         SN_general_Ave, SN_work_Ave, ProReas, ConReas, State)

# now, merge with the long formatted reasons data and merge by participant_id

against_formatted <- merge(x = against_long, 
                           y = against_to_merge, 
                           by = "participant_id", 
                           all.y = T)


# finally remove rows with NA values 

against_long <- against_formatted %>%
  filter(!is.na(Against_Stated)) 


# add row_id, which will be used after creating topic model
# to re-assign the topic model output back to the unique row id 
against_long$row_id <- paste(1:nrow(against_long))
```


STM processing and model diagnostics 
```{r}

# renmae Against_Stated to 'document' for tm processing 
against_long <- against_long %>%
  rename(document = Against_Stated) 

# uses textProcessor() function from tm package 
against_processed <- textProcessor(against_long$document, metadata = against_long)

against_out <- prepDocuments(against_processed$documents, 
                             against_processed$vocab, 
                             against_processed$meta)


# two methods for identifying model fit 

# identify best k = c(2:10)
against_ntopics <- searchK(against_out$documents, against_out$vocab, K = c(2, 3, 4, 5, 6, 7, 8, 9, 10), data = meta)
plot(against_ntopics)

# show comparison of fit statistics for k = c(2:4)
against_ntopics <- searchK(against_out$documents, against_out$vocab, K = c(2, 3, 4), data = meta)
# plot fit statistic values for solutions on reasons against data 
plot(against_ntopics)

# grab fit statistics for topic model solutions on reasons against data 
against_ntopics$results

```


2-topic model solution for reasons against data 

```{r against-2-topic-solution}

against_fit_two <- stm(against_out$documents, # the documents
            against_out$vocab, # the words
            K = 2, # 3 topics
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = against_out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral")  



# highest word probabilities bar chart 
td_beta <- tidy(against_fit_two) 
word_probs(td_beta)

# look at highest word probability
labelTopics(against_fit_two)

# top terms conditioning on exclusivity of terms by between topics 
plot.STM(against_fit_two, type = "labels", labeltype = c("frex"), main = "2-Topic Solution: Exclusivity")  

# perpsective plot: can add additional context on differentiation b/w two specific topics in the solution
plot.STM(against_fit_two, type = "perspectives", topics = c(1,2))   

# expected proportion of topics in all reasons for data 
plot.STM(against_fit_two, type = "summary", main = "2-Topic Expected Proportion")  

# look at top words by category
labelTopics(against_fit_two)

```



2-topic solution, Reasons Against - Estimated Effects 
```{r}

# Attitude 
against_est_two_1 <- estimateEffect(1:2 ~ Att_Ave, stmobj = against_fit_two, metadata = against_out$meta, 
                                    uncertainty = "None")    

summary(against_est_two_1) 

# Subjective Norm 

against_est_two_2 <- estimateEffect(1:2 ~ SN_Ave, stmobj = against_fit_two, metadata = against_out$meta, 
                       uncertainty = "None")   
summary(against_est_two_2)

# Perceived Control

against_est_two_3 <- estimateEffect(1:2 ~ PC_Ave, stmobj = against_fit_two, metadata = against_out$meta, 
                       uncertainty = "None")   
summary(against_est_two_3)
```



3-topic model solution for reasons against data 

```{r}

against_fit_three <- stm(against_out$documents, # the documents
            against_out$vocab, # the words
            K = 3, # 3 topics
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = against_out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral")  



# highest word probabilities bar chart 
td_beta <- tidy(against_fit_three) 
word_probs(td_beta)

# look at highest word probability
labelTopics(against_fit_three)

# top terms conditioning on exclusivity of terms by between topics 
plot.STM(against_fit_three, type = "labels", labeltype = c("frex"), main = "3-Topic Solution: Exclusivity")  

# perpsective plot: can add additional context on differentiation b/w two specific topics in the solution
plot.STM(against_fit_three, type = "perspectives", topics = c(1,2))   

# expected proportion of topics in all reasons for data 
plot.STM(against_fit_three, type = "summary", main = "3-Topic Expected Proportion")  

# look at top words by category
labelTopics(against_fit_three)

# custom proption of topic in overall corpus for 3-topic model 

reasons_against_prop <- make.dt(against_fit_three)

summarize_all(reasons_against_prop, mean) 

td_beta

#write.csv(td_beta, "against_fit_k3_beta.csv") 

```


Reasons Against Estimated Effects 3-topic solution 

```{r}

# Attitude 
set.seed(1061) 
against_est_three_1 <- estimateEffect(1:3 ~ Att_Ave, stmobj = against_fit_three, metadata = against_out$meta, 
                                    uncertainty = "None")    

summary(against_est_three_1) 

# Subjective Norm 

against_est_three_2 <- estimateEffect(1:3 ~ SN_Ave, stmobj = against_fit_three, metadata = against_out$meta, 
                       uncertainty = "None")   
summary(against_est_three_2)

# Perceived Control

against_est_three_3 <- estimateEffect(1:3 ~ PC_Ave, stmobj = against_fit_three, metadata = against_out$meta, 
                       uncertainty = "None")   
summary(against_est_three_3)


# show the estimated relationship between Against Topic 3 and PC 
plot(against_est_three_1, covariate = "Att_Ave", model = against_fit_three,
     method = "continuous", xlab = "Attitude", topics = c(1, 2), 
     main = "Attitude by Topic 1 and Topic 2 \n in Reasons Against")  


```


4-topic model solution for reasons against data 


```{r stm-against-4-topic-model}

against_fit_four <- stm(against_out$documents, # the documents
            against_out$vocab, # the words
            K = 4, 
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = against_out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral")  



# highest word probabilities bar chart 
td_beta <- tidy(against_fit_four) 
word_probs(td_beta)

# look at highest word probability
labelTopics(against_fit_four)

# top terms conditioning on exclusivity of terms by between topics 
plot.STM(against_fit_four, type = "labels", labeltype = c("frex"), main = "4-Topic Solution: Exclusivity")  

# perpsective plot: can add additional context on differentiation b/w two specific topics in the solution
plot.STM(against_fit_four, type = "perspectives", topics = c(1,2))   

# expected proportion of topics in all reasons for data 
plot.STM(against_fit_four, type = "summary", main = "4-Topic Expected Proportion")  

# look at top words by category
labelTopics(against_fit_four)
```


Reasons Against Estimated Effects 3-topic solution 

```{r 4-topic-estimated-effects}

# Attitude 
set.seed(10826) 
against_est_four_1 <- estimateEffect(1:4 ~ Att_Ave, stmobj = against_fit_four, metadata = against_out$meta, 
                                    uncertainty = "None")    

summary(against_est_four_1) 

# Subjective Norm 

against_est_four_2 <- estimateEffect(1:4 ~ SN_Ave, stmobj = against_fit_four, metadata = against_out$meta, 
                       uncertainty = "None")   
summary(against_est_four_2)

# Perceived Control

against_est_four_3 <- estimateEffect(1:4 ~ PC_Ave, stmobj = against_fit_four, metadata = against_out$meta, 
                       uncertainty = "None")   
summary(against_est_four_3)


# show the estimated relationship between Against Topic 3 and PC 
plot(against_est_four_1, covariate = "Att_Ave", model = against_fit_four,
     method = "continuous", xlab = "Attitude", topics = c(3), 
     main = "Attitude by Topic 3 \n in Reasons Against")  


```



# Descriptive Statistics 



## Counts and summaries 

Median character count, total words, total unique words ... 




# N-Grams 


## Most common n-grams (full corpus) 




## N-grams by focal groups

The following code chunk visualizes different sets of top word co-occurances by groups: 





```{r}
# bigrams for ___ 
occupation_bigrams %>%
  filter(occupation_group == "group1") %>% # to update filter by
  count(bigram) %>% arrange(desc(n)) %>% head(5) %>%
  ggplot(aes(x = reorder(bigram, n), y = n, fill = bigram)) + geom_bar(stat = "identity") + 
  coord_flip() + theme(legend.position = "none") + 
  xlab("Bigram") + ylab("Freq.") 



```



# Sentiment Analysis 

Classify response sentiment using the pre-trained Vader dictionary in Python. Then, return to R for subsequent analysis. 

```{python}
# store df in python from r 
# to update with new data  

df_complete = r.df_complete

# libaries 
import numpy as np
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import WordPunctTokenizer
from nltk.stem.wordnet import WordNetLemmatizer
import matplotlib.pyplot as plt


from nltk.sentiment.vader import SentimentIntensityAnalyzer
# Creating instance
analyzer = SentimentIntensityAnalyzer()
april_sent = april_complete.copy(deep = True)
april_sent['Positive'] = [analyzer.polarity_scores(x)['pos'] for x in april_sent['Q16_workload']]
april_sent['Negative'] = [analyzer.polarity_scores(x)['neg'] for x in april_sent['Q16_workload']]
april_sent['Neutral'] = [analyzer.polarity_scores(x)['neu'] for x in april_sent['Q16_workload']]
april_sent['Compound'] = [analyzer.polarity_scores(x)['compound'] for x in april_sent['Q16_workload']]
april_sent['Positive'] = april_sent['Positive'].round(2)*100
april_sent['Negative'] = april_sent['Negative'].round(2)*100
april_sent['Neutral'] = april_sent['Neutral'].round(2)*100
april_sent['Compound'] = april_sent['Compound'].round(2)*100
april_sent.head()
```

Following classification in Python, import back into R. 

```{r}

# import df from python
# to update with new data

df_sentiment <- py$df_sent

april_sentiment %>%
  select(mseo_group, supervisory, center, Positive, Negative, 
         Neutral, Compound) 

lm(Positive ~  group_1 + supervisory, 
   data = april_sentiment)  %>% summary() 

april_sentiment %>%
  ggplot(aes(x = Negative, y = Positive, 
             color = group_1)) + geom_point() + 
  geom_smooth(method = "lm") + xlim(0,50) + ylim(0,60) 
  
```


