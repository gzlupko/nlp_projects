---
title: "Gian Zlupko Empirical Paper"
output: html_notebook
---

# Empirical Paper 
## Using Natural Language Processing to Identify the Substantive Reasoning Underlying Behavior: An Application to Employeesâ€™ Behavior to Work-From-Home During COVID-19

[Study overview]


## Libraries and Setup 

```{r Libraries-and-Path, include = F, message = F}

library(tidytext) 
library(tidyverse)
library(topicmodels) 
library(ldatuning)
library(textstem) 
library(SnowballC) 
library(tm)
library(car) 
library(stm) # structural topic model 
library(wesanderson)
library(readxl) # to import xlsx file 
library(xlsx) # to write to xlsx file 
library(psych) # useful for descriptive stats 
library(jtools)  # has theme_apa() function for formatting ggplot objects in APA style 
```

## Data Cleaning 

```{r Data-Cleaning}
# set temporary working directory 
setwd("/Users/gianzlupko/Desktop/Workgroup/dnl_nlp/Studies/empirical_paper")
# load raw data and begin conversion to long data format 
dat <- read_excel("remote_work_empirical_data.xlsx") 

# store a raw data set to for backup 
raw_data <- dat

# view data set 
head(dat) 

```

Six individuals that did not pass attention checks were manually removed before importing the data set. 45 did not complete the survey. Their rows were removed. Regarding T2 retention rate, 65 individuals of the 399 who completed the survey at T1 did not return. Thus, we observed a retention rate of 83.7%. 

```{r}
# remove individuals that did not complete the survey; 45 did not complete the survey 
dat_1 <- dat %>% 
  filter(!Finished == 0) 


# how many individuals (who finished T1) completed the survey at T2? - We see that 65 did not complete T2. 
dat_1 %>% 
  filter(is.na(T2_JobSat_Ave)) 

T2_retention_rate <- (399-65)/399*100
T2_retention_rate

# T2 sample size 
399 - 65

```

## Reshape data from wide to long format 

First I clean column names and assign a unique ID to participants. Then, I break out the full data set into a reasons for the decision and reasons against the decision. I apply topic modeling and sentiment analysis to these data sets individually. Then, after reporting on results from the text analysis, I use the participant's unique ID that I created to bring the NLP features (topic proportions and sentiment scores) back into the original data set to estimate regression equations containing the NLP features with the additional variables in the data set: T2 behavior (DV), burnout (DV), and job satisfaction (covariate). 

By reshaping the data, converting from wide to long format, each reason is first analyzed 

```{r}
# rename columns and store in 
dat_clean <- dat_1 %>%
   rename(Decision = Q4, Reason1 = Q12_1, Reason2 = Q12_2, Reason3 = Q12_3, 
          Reason4 = Q12_4, Reason5 = Q12_5, Reason6 = Q12_6, Reason7 = Q12_7, 
          Reason8 = Q12_8, Reason9 = Q12_9, Reason10 = Q12_10, Against1 = Q33_1, Against2 = Q33_2, Against3 = Q33_3, 
          Against4 = Q33_4, Against5 = Q33_5, Against6 = Q33_6, Against7 = Q33_7, 
          Against8 = Q33_8, Against9 = Q33_9, Against10 = Q33_10, 
          job_sat = T2_JobSat_Ave, behavior_ave = T2_Behavior_3Item_Ave, burnout = T2_Burnout_4Item_Ave)  


# add unique participant id with simple sequence 1:N; this is used to later link transformed data back to individual
dat_clean <- dat_clean %>%
  mutate(participant_id = 1:length(Decision))


# create one data set for reasons for containing reasons data and participant ID 
# used to convert from wide to long format 

reasons_data <- dat_clean %>%
  select(participant_id, Reason1, Reason2, Reason3, Reason3, 
         Reason4, Reason5, Reason6, Reason7, Reason8, Reason9, Reason10)  

# first convert reasons to long data format
reasons_long <- reasons_data %>%
  gather(key = "Reason", 
         value = "Reason_Stated", c(-participant_id)) 


# next create a subset of the original data without the reasons data
# instead, retain only the BRT and decision variable scores to reattach
# to the long data frame

scores_to_merge <- dat_clean %>%
  select(participant_id, behavior_ave, burnout, job_sat)

# now, merge with the long formatted reasons data and merge by participant_id

reasons_formatted <- merge(x = reasons_long, 
                           y = scores_to_merge, 
                           by = "participant_id", 
                           all.y = T)

reasons_formatted 

# finally remove rows with NA values 
reasons_long <- reasons_formatted %>%
  filter(!is.na(Reason_Stated)) 

# add row_id, which will be used after creating topic model
# to re-assign the topic model output back to the unique row id 
reasons_long$row_id <- paste(1:nrow(reasons_long))


# write transformed data set to local to store a copy
#write.xlsx(reasons_long, "reasons_long_formatted.xlsx") 

```

## Reshape Reasons Against Data 

```{r}

# subset dat_clean for reasons against data 

against_data <- dat_clean %>% 
  select(participant_id, Against1:Against10)  

# convert against data to long 

# first convert reasons to long data format
against_long <- against_data %>%
  gather(key = "Against", 
         value = "Against_Stated", c(-participant_id)) 


# merge against data in long format with the DVs in 'scores_to_merge' 
against_formatted <- merge(x = against_long, 
                           y = scores_to_merge, 
                           by = "participant_id", 
                           all.y = T)

# 

# finally remove rows with NA values 
against_long <- against_formatted %>%
  filter(!is.na(Against_Stated)) 

# add row_id, which will be used after creating topic model
# to re-assign the topic model output back to the unique row id 
against_long$row_id <- paste(1:nrow(against_long))

# write transformed data set to local to store a copy
#write.xlsx(against_long, "against_long_formatted.xlsx") 

against_long

```





# Text Descriptives

First, of interest to Behavioral Reasoning Theory (BRT) research, I examine the number of reasons for the decision stated by the participants. Next, before calculating additional descriptive statistics, I need to transform the text data into 

### Number of Reasons Stated 
Examining the raw text. 

* 4.7 reasons per participant on average  
    + SD = 2.1 
    + median = 4 
    + min = 1, max = 10 
    
* Median character count is 21 
    + mean = 24.6 
    + SD = 16.32 
    + min = 3, max = 173 
    + 
  
```{r}

# how many reasons provided by participants on average? 

reasons_for_count <- reasons_long %>% 
  group_by(participant_id) %>% 
  count(participant_id) 

mean(reasons_for_count$n) 
skew(reasons_for_count) 

# descriptives on reason count data 
psych::describe(reasons_for_count$n)

# word counts and averages 

# calculate average characters for employee responses in Q16 

reasons_char_counts <- reasons_long %>%
  mutate(num = nchar(Reason_Stated)) %>%
   select(num, Reason_Stated)

reasons_char_counts %>% summary() 

describe(reasons_char_counts$num) 

describe(reasons_for_count$n) 



```
### Number of Reasons Against Stated 


```{r}
# how many against provided by participants on average? 

against_count <- against_long %>% 
  group_by(participant_id) %>% 
  count(participant_id) 

describe(against_count$n) 

```


# Text Features 

Generate text features like sentiment scores and point-of-speech metrics using the `textfeatures` package in R. I ran this function before pre-processing as these features may benefit from more information captured by stop words and punctuation. First, I do this for the reasons for data below. 

```{r}
library(textfeatures) 

# run textfeatures workhorse function

reasons_for_features <- textfeatures(reasons_long$Reason_Stated,
                                     sentiment = TRUE, 
                                     word_dims = FALSE, 
                                  normalize = FALSE) 

reasons_long 
# inspect newly generated features 
reasons_for_features <- reasons_for_features %>% 
  select(n_chars, n_uq_chars, n_words, n_uq_words, sent_afinn, sent_bing, 
         sent_syuzhet, sent_vader, n_polite, n_first_person, n_first_personp, n_second_person, n_second_personp, n_third_person) 

reasons_for_features <- reasons_for_features %>% 
  mutate(row_id = seq(nrow(reasons_for_features))) 

# convert reasons long row_id column to numeric to perform data join 
reasons_long$row_id <- as.numeric(reasons_long$row_id) 

# join text features to data with 

reasons_with_features <- reasons_long %>% 
  left_join(reasons_for_features) 

# inspect newly created data set with text features added to reasons data
reasons_with_features 

# save a copy of this data set to local 
#write.xlsx(reasons_with_features, "reasons_for_with_features.xlsx")

reasons_with_features 

```


Next I do the same for the reasons against data 

```{r}

# run textfeatures workhorse function

against_features <- textfeatures(against_long$Against_Stated,
                                     sentiment = TRUE, 
                                     word_dims = FALSE, 
                                  normalize = FALSE) 


# inspect newly generated features 
agaist_features <- against_features %>% 
  select(n_chars, n_uq_chars, n_words, n_uq_words, sent_afinn, sent_bing, 
         sent_syuzhet, sent_vader, n_polite, n_first_person, n_first_personp, n_second_person, n_second_personp, n_third_person) 

against_features <- against_features %>% 
  mutate(row_id = seq(nrow(against_features))) 

# convert reasons long row_id column to numeric to perform data join 
against_long$row_id <- as.numeric(against_long$row_id) 

# join text features to data with 

against_with_features <- against_long %>% 
  left_join(against_features) 

# inspect newly created data set with text features added to reasons data
against_with_features 

# save a copy of this data set to local 
#write.xlsx(against_with_features, "against_with_features.xlsx")


```






## Text Variables - Descriptive Stats 

Run descriptive summaries of text variables 

```{r}
# summary stats on text data 
reasons_for_descriptives <- describe(reasons_with_features)

# save text descriptives 
#write.xlsx(reasons_for_descriptives, "reasons_for_descriptives.xlsx") 

reasons_for_descriptives
  
```

Descriptive stats on against text variables

```{r}
# summary stats on text data 
against_descriptives <- describe(against_with_features)

# save text descriptives 
#write.xlsx(against_descriptives, "against_descriptives.xlsx") 

against_descriptives
  
```



# Text Pre-processing 

The following code uses the `textProcessor()` function in the `stm` package, which


```{r}

# rename 'Reasons_Stated' column to 'document', which the topic model takes
reasons_with_features <- reasons_with_features %>%
  rename(document = Reason_Stated) 

# run data frame through basic pre-processing: remove punctuation, 
# stop words, custom stop words, numbers, and stem.

processed <- textProcessor(documents = reasons_with_features$document, 
                           metadata = reasons_with_features)   

# store processed objects to inspect pre-processing applied 

docs <- processed$documents
vocab <- processed$vocab
meta <- processed

```


# Topic Modeling



## Thresholds 

By default, `prepDocuments` removes words that appear in only one document. Users
can see which documents (e.g. survey responses) had terms removed. If any responses
had no words or the default threshold (e.g. 1), then those are removed as well. Users
can also examine which documents were dropped. 

Below, the `plotRemoved` function allows a researcher to iteratively
tune a threshold parameter to adjust the minimum number of responses that 
a word needs to appear in to be included in subsequent topic modeling.

As an example, requiring that terms appear in at least 5 documents would remove 
4,103 of 5,442 words in the corpus after pre-processing. Depending on the analysts' goal, 
this could be good or bad. An analyst may consider fine tuning the minimum threshold 
parameter in an iterative fashion to achieve desirable results.  

Upper thresholds can also be passed through the `prepDocuments` function 
to remove words that appear in too many responses. 


Currently setting lower threshold to 3, which may be smaller lower threshold for other studies, but is intended to be more inclusive in this study given the short text responses. 
```{r}

# default threshold: 1
out <- prepDocuments(processed$documents, processed$vocab)

# explore minimum threshold 
plotRemoved(processed$documents, lower.thresh = seq(1, 50, by = 1)) 

# set lower threshold of 5, requiring words to appear in at least 5 reasons stated in order to be included in the analysis

out <- prepDocuments(documents = processed$documents,
              vocab = processed$vocab,
              meta = processed$meta, 
              lower.thresh = 3) 


```


## Model Diagnostics - Reasons For

There are multiple evaluation strategies available. First, researchers can fit topic models across k topics and evaluate model fit and human interpretability based on various criteria developed by researchers. Below  to consider

The function `searchK` allows users to use a data-driven approach to identifying
the number of latent topics. Specifically, the

```{r}

# run function to calculate diagnostic stats for different k-models 
ntopics <- searchK(out$documents, out$vocab, K = c(2,3,4,5,6,7,8,9,10), 
                   data = meta)

# plot the topic diagnostic outputs 
plot(ntopics) 

# store diagnostic statistics in data set 
diag_stat_reasons <- ntopics$results 

diag_stat_reasons <- diag_stat_reasons %>% rename(k_topics = K) 

# visualize semantic coherence across models 
diag_stat_reasons %>%
  tail(10) %>%
  ggplot( aes(x=k_topics, y=semcoh)) +
    geom_line() +
    geom_point() + ylab("Semantic Coherence") + xlab("k - Number of Topics")


# visualize exclusivity x semantic coherence

diag_stat_reasons
```


Fixed k, select model 

```{r}

stm_select_mod <- selectModel(out$documents, out$vocab, K = 3, max.em.its = 75, data = out$meta, runs = 20, seed = 10210)  

# average semantic coherence score and exclusivity 
# model numerals are shown. Top 4 models are plotted. Models in the top right are best (e.g. high exclusivity and coherence closer to zero)
plotModels(stm_select_mod, pch = c(4,5,6,7),legend.position = "bottomright")

# return raw values of top three models for semantic coherence and generate model avgs 
stm_select_mod$semcoh
select_k3 <- t(as.data.frame(stm_select_mod$semcoh, col.names = c("Mod1", "Mod2", "Mod3", "Mod4"))) 
select_k3 <- as.data.frame(select_k3) 

k3_semantic_coherence <- select_k3 %>% 
  mutate(model_id = row_number()) %>%
  rename(run1 = V1, run2 = V2, run3 = V3) %>% 
  group_by(model_id) %>% 
  mutate(mean_sem_coh = mean(c(run1, run2, run3)))  


# return raw values of top three models for semantic coherence and generate model avgs 
stm_select_mod$exclusivity
k3_exclusivity <- t(as.data.frame(stm_select_mod$exclusivity, col.names = c("Mod1", "Mod2", "Mod3", "Mod4"))) 
k3_exclusivity <- as.data.frame(k3_exclusivity) 

k3_exclusivity <- k3_exclusivity %>% 
  mutate(model_id = row_number()) %>%
  rename(run1 = V1, run2 = V2, run3 = V3) %>% 
  group_by(model_id) %>% 
  mutate(mean_exclusivity = mean(c(run1, run2, run3)))  

k3_exclusivity

# merge data sets by model ID 

k3_sem_exclusivity <- k3_semantic_coherence %>% 
  left_join(k3_exclusivity, by = "model_id") %>%
  select(model_id, mean_sem_coh, mean_exclusivity) %>% 
  mutate(model_id = as.factor(model_id)) 

k3_sem_exclusivity

# plot sem coherence and exclusivity for the generated k = 3 topic models 
k3_sem_exclusivity %>% 
  ggplot(aes(x = mean_sem_coh, y = mean_exclusivity, color = model_id, shape = model_id)) + geom_point(size = 4) + labs(x = "Semantic Coherence", y = "Exclusivity") +  scale_shape_manual(values=c(7, 3, 17, 18)) 

# select Model 1, which is seen as being the most optimized on exclusivity and coherence 
# numbers represent the average for each model and dots represent individual scores 
reasons_for_k3 <- stm_select_mod$runout[[1]]


```





## Model Diagnostics - Reasons Against 

```{r}

# rename 'Reasons_Stated' column to 'document', which the topic model takes
against_with_features <- against_with_features %>%
  rename(document = Against_Stated) 

# run data frame through basic pre-processing: remove punctuation, 
# stop words, custom stop words, numbers, and stem.
against_processed <- textProcessor(documents = against_with_features$document, 
                           metadata = against_with_features)   

# store processed objects to inspect pre-processing applied 
against_docs <- against_processed$documents
against_vocab <- against_processed$vocab
against_meta <- against_processed


# default threshold: 1
against_out <- prepDocuments(against_processed$documents, against_processed$vocab)


# set lower threshold of 5, requiring words to appear in at least 5 reasons stated in order to be included in the analysis

against_out <- prepDocuments(documents = against_processed$documents,
              vocab = against_processed$vocab,
              meta = against_processed$meta, 
              lower.thresh = 1) #this is default lower threshold  


# run function to calculate diagnostic stats for different k-models 
against_ntopics <- searchK(against_out$documents, out$vocab, 
                           K = c(2,3,4,5,6,7,8,9,10), data = against_meta)

# plot the topic diagnostic outputs 
plot(against_ntopics) 

# store diagnostic statistics in data set 
against_diag <- against_ntopics$results 
against_diag <- against_diag %>% rename(k_topics = K) 

# semantic coherence for reasons against data 
against_diag %>%
  tail(10) %>%
  ggplot( aes(x=k_topics, y=semcoh)) +
    geom_line() +
    geom_point() + ylab("Semantic Coherence") + xlab("k - Number of Topics") + 
  theme_apa() 


```





# Top Terms 
## Reasons For 


```{r}
# fit k = 3 topic model for reasons for data 
reasons_for_k3

# Turn the STM object into a data frame. This is necessary so that we can work with it.
reasonsk3_td_beta <- tidy(reasons_for_k3)

# create re-useable function for subsequent topic model solutions 
# bar chart uses wesanderson color palettes
word_probs_reasons_for <- function(reasonsk3_td_beta) { 
  
  reasonsk3_td_beta %>%
  # Group by topic
  group_by(topic) %>%
  # Take the top 10 based on beta
  top_n(10, beta) %>%
  # Ungroup
  ungroup() %>%
  # Generate the variables topic and term
  dplyr::mutate(topic = paste0(topic),
                term = reorder_within(term, beta, topic)) %>%
  # And plot it
  ggplot() +
  # Using a bar plot with the terms on the x-axis, the beta on the y-axis, filled by topic
  geom_col(aes(x = term, y = beta, fill = as.factor(topic)),
           alpha = 0.8,
           show.legend = FALSE) +
  # Do a facet_wrap by topic
  facet_wrap(~ topic, scales = "free_y") +
  # And flip the plot
  coord_flip() +
  scale_x_reordered() +
  # Label the x-axis, y-axis, as well as title
  labs(x = "Word",
    y = expression(beta),
    title = "Highest word probabilities for each topic") +
  # And finally define the colors
  scale_fill_manual(values = wes_palette("Darjeeling1")) + theme_apa() 
  
  
}

# generate bar chart for 3 topic solution
word_probs_reasons_for(reasonsk3_td_beta) 


```




## Select Model Reasons Against 



```{r}

against_stm_select_mod <- selectModel(against_out$documents, against_out$vocab, K = 4, max.em.its = 75, data = against_out$meta, runs = 20, seed = 10210)  

# average semantic coherence score and exclusivity 
# model numerals are shown. Top 4 models are plotted. Models in the top right are best (e.g. high exclusivity and coherence closer to zero)
plotModels(against_stm_select_mod, pch = c(4,5,6,7),legend.position = "bottomright")


# return raw values of top three models for semantic coherence and generate model avgs 
against_stm_select_mod$semcoh
against_select_k4 <- t(as.data.frame(against_stm_select_mod$semcoh, col.names = c("Mod1", "Mod2", "Mod3", "Mod4"))) 
against_select_k4 <- as.data.frame(against_select_k4) 

against_k4_semantic_coherence <- against_select_k4 %>% 
  mutate(model_id = row_number()) %>%
  rename(run1 = V1, run2 = V2, run3 = V3) %>% 
  group_by(model_id) %>% 
  mutate(mean_sem_coh = mean(c(run1, run2, run3)))  



# return raw values of top three models for semantic coherence and generate model avgs 
against_stm_select_mod$exclusivity
against_k4_exclusivity <- t(as.data.frame(against_stm_select_mod$exclusivity, col.names = c("Mod1", "Mod2", "Mod3", "Mod4"))) 
against_k4_exclusivity <- as.data.frame(against_k4_exclusivity) 

against_k4_exclusivity <- against_k4_exclusivity %>% 
  mutate(model_id = row_number()) %>%
  rename(run1 = V1, run2 = V2, run3 = V3) %>% 
  group_by(model_id) %>% 
  mutate(mean_exclusivity = mean(c(run1, run2, run3)))  

against_k4_exclusivity

# merge data sets by model ID 

against_k4_sem_exclusivity <- against_k4_semantic_coherence %>% 
  left_join(against_k4_exclusivity, by = "model_id") %>%
  select(model_id, mean_sem_coh, mean_exclusivity) %>% 
  mutate(model_id = as.factor(model_id)) 

against_k4_sem_exclusivity

# plot sem coherence and exclusivity for the generated k = 3 topic models 
against_k4_sem_exclusivity %>% 
  ggplot(aes(x = mean_sem_coh, y = mean_exclusivity, color = model_id, shape = model_id)) + geom_point(size = 4) + labs(x = "Semantic Coherence", y = "Exclusivity") +  scale_shape_manual(values=c(7, 3, 17, 18)) + theme_apa() 

# select Model 1, which is seen as being the most optimized on exclusivity and coherence 
# numbers represent the average for each model and dots represent individual scores 
against_reasons_for_k4 <- against_stm_select_mod$runout[[4]]

```



```{r}

# reasons against k = 4 topic model 
against_reasons_for_k4

against_tidy <- tidy(against_reasons_for_k4) 

word_probs_against <- function(against_tidy) { 
  
  against_tidy %>%
  # Group by topic
  group_by(topic) %>%
  # Take the top 10 based on beta
  top_n(10, beta) %>%
  # Ungroup
  ungroup() %>%
  # Generate the variables topic and term
  dplyr::mutate(topic = paste0(topic),
                term = reorder_within(term, beta, topic)) %>%
  # And plot it
  ggplot() +
  # Using a bar plot with the terms on the x-axis, the beta on the y-axis, filled by topic
  geom_col(aes(x = term, y = beta, fill = as.factor(topic)),
           alpha = 0.8,
           show.legend = FALSE) +
  # Do a facet_wrap by topic
  facet_wrap(~ topic, scales = "free_y") +
  # And flip the plot
  coord_flip() +
  scale_x_reordered() +
  # Label the x-axis, y-axis, as well as title
  labs(x = "Word",
    y = expression(beta),
    title = "Highest word probabilities for each topic") +
  # And finally define the colors
  scale_fill_manual(values = wes_palette("Darjeeling1"))
  
  
}

# generate bar chart for 4 topic solution
word_probs_against(against_tidy) 

 
```



# Assign Topics to Study Data 

STM to data table format. The steps below export topic proportions from the topic model and assign them to the original data as proportions that can subsequently used to model their relationship to other variables in the data set. 

```{r}

reasons_for_k3

# generate data table; using meta we can rejoin to the original data set 
reasons_for_topic_props <- make.dt(reasons_for_k3, meta = out$meta)
reasons_for_topic_props

# write to excel 
#write.xlsx(reasons_for_topic_props, "all_variables_reasons_for.xlsx") 

# create a data table for the reasons against data that can then be joined to the above set 
against_topic_props <- make.dt(against_reasons_for_k4, against_out$meta)  

against_topic_props
#write.xlsx(against_topic_props, "reasons_against_all_variables.xlsx")


```



# Correlation Matrices 
## Reasons For 

```{r}

reasons_cor_data <- reasons_for_topic_props %>% 
  select(Topic1, Topic2, Topic3, behavior_ave, burnout, job_sat, 
         n_words, n_uq_words, sent_vader, n_first_person, n_first_personp,
         n_second_person, n_second_personp, n_third_person)


reasons_cor_matrix <- cor(reasons_cor_data, use = "complete.obs") 
#write.xlsx(reasons_cor_matrix, "reasons_for_correlation_matrix.xlsx") 
```


# Load cleaned and processed data here 
Load data that was already processed and in which topic proportions and other text features were already applied; its stored in local 


```{r}
getwd() 

reasons_for_data <- read_excel("all_variables_reasons_for.xlsx") 

# log transform the topic proportions per Doldor et al. (2017)
reasons_for_data[,3:5] <- log(reasons_for_data[,3:5]) 
reasons_for_data

```



# Exemplar responses

```{r}
reasons_for_data <- read_excel("all_variables_reasons_for.xlsx") 

# build an exemplar df for each topic 
# create and repeat for each topic in reasons for 
reasons_for_topic1_examples <- reasons_for_data %>% 
  arrange(desc(Topic1)) %>%
  filter(Topic1 > .54) %>% select(Topic1:Topic3, document) 

#write.xlsx(reasons_for_topic1_examples, "reasons_for_topic1_examples.xlsx")

reasons_for_topic2_examples <- reasons_for_data %>% 
  arrange(desc(Topic2)) %>% 
  filter(Topic2 > .41) %>% select(Topic1:Topic3, document) 
#write.xlsx(reasons_for_topic2_examples, "reasons_for_topic2_examples.xlsx")


reasons_for_topic3_examples <- reasons_for_data %>% 
  arrange(desc(Topic3)) %>% 
  filter(Topic3 > .54) %>% select(Topic1:Topic3, document)
#write.xlsx(reasons_for_topic3_examples, "reasons_for_topic3_examples.xlsx")
```


# Reasons against exemplar responses 

```{r}
reasons_against_full <- read_excel("reasons_against_all_variables.xlsx") 

# filter by greatest proportion for each topic. Store to file for formatting in Excel 

against_topic1_responses <- reasons_against_full %>% 
  arrange(desc(Topic1)) %>% select(Topic1:Topic4, document) %>% 
  filter(Topic1 > .500) 
#write.xlsx(against_topic1_responses, "against_topic1_responses.xlsx")


against_topic2_responses <- reasons_against_full %>% 
  arrange(desc(Topic2)) %>% select(Topic1:Topic4, document) %>%
  filter(Topic2 > .49)
#write.xlsx(against_topic2_responses, "against_topic2_responses.xlsx")

against_topic3_responses <- reasons_against_full %>% 
  arrange(desc(Topic3)) %>%
  filter(Topic3 > .35) %>% select(Topic1:Topic4, document)
#write.xlsx(against_topic3_responses, "against_topic3_responses.xlsx") 

against_topic4_responses <- reasons_against_full %>% 
  arrange(desc(Topic4)) %>% 
  filter(Topic4 > .34) %>% select(Topic1:Topic4, document)
#write.xlsx(against_topic4_responses, "against_topic4_responses.xlsx") 
```




Regression with log transformed 

Results showed no difference with the log transformed data vs. the scale transformation used below. 

```{r}

# hierarchical MR 
# mod1 = job sat and burnout 
mod1 <- lm(burnout ~ job_sat, data = reasons_for_data)
mod1 %>% summary() 

# mod2 = topic proportions 

mod2_topics <- lm(burnout ~ job_sat + Topic1 + Topic2 + Topic3 + sent_vader, data = reasons_for_data)
summary(mod2_topics) 

anova(mod1, mod2_topics) 


```




# Regression Analyses 

### Reasons For

```{r}

reasons_for_topic_props
# scale select columns in df 
reasons_for_scaled <- reasons_for_topic_props %>% 
   mutate_at(c(2:4, 8:10, 12:25), funs(c(scale(.))))

# hierarchical MR 
# mod1 = job sat and burnout 
mod1 <- lm(behavior_ave ~ burnout + job_sat, data = reasons_for_scaled)
mod1 %>% summary() 

# mod2 = text features added 

mod2_reasons_for <- lm(behavior_ave ~ burnout + job_sat + sent_vader + n_words + n_first_person + n_first_personp + n_second_personp  + n_third_person, data = reasons_for_scaled)
mod2 %>% summary() 


# added text features did account for sig more variance in behavior at T2, though additional # variance accounted for was small 

anova(mod1, mod2_reasons_for) 

# export regression analysis in apa format 
library(apaTables) 
reasons_for_reg_table <- apa.reg.table(mod2_reasons_for, filename = "reasons_for_reg_table.doc") 


```


### Reasons Against 
#### Regression Testing 

```{r}

against_topic_props

# scale select columns in data needed for regression
against_mlr_data <- against_topic_props %>% 
  select(Topic1:Topic4, behavior_ave:job_sat, n_chars:n_third_person)

against_scaled <- data.frame(scale(against_mlr_data)) 

 
# regression 1; already created earlier 
mod1

# model2: topics against + text features 

mod2_reasons_against <- lm(behavior_ave ~ job_sat + burnout + Topic1 + Topic2 + Topic3 + sent_vader + n_words + n_first_person + n_first_personp + n_second_personp + n_third_person, data = against_scaled)
summary(mod2) 


# export regression table output; unable to save as an excel type with apaTables 
reasons_against_reg_table <- apa.reg.table(mod2_reasons_against, filename = "against_reg_table.doc") 


```




# Multi-level 
# Test for multilevel 
## Reasons for data


```{r, message = FALSE}
reasons_mlm_data <- reasons_for_topic_props %>% 
  select(Topic1, Topic2, Topic3, behavior_ave, sent_vader)
participants <- reasons_for_topic_props %>% select(participant_id) 


reasons_mlm_data <- as.data.frame(scale(reasons_mlm_data)) 
reasons_mlm_data <- cbind(reasons_mlm_data, participants) 
reasons_mlm_data <- data.frame(reasons_mlm_data)

# mlm test 
lmer(data = reasons_mlm_data, behavior_ave ~ Topic1 + Topic2 +sent_vader + (1 | participant_id)) %>% summary() 

```








