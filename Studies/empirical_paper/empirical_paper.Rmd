---
title: "Gian Zlupko Empirical Paper"
output: html_notebook
---

# Empirical Paper 
## Using Natural Language Processing to Identify the Substantive Reasoning Underlying Behavior: An Application to Employeesâ€™ Behavior to Work-From-Home During COVID-19

[Study overview]


## Libraries and Setup 

```{r Libraries-and-Path, include = F, message = F}

library(tidytext) 
library(tidyverse)
library(topicmodels) 
library(ldatuning)
library(textstem) 
library(SnowballC) 
library(tm)
library(car) 
library(stm) # structural topic model 
library(wesanderson)
library(readxl) # to import xlsx file 
library(xlsx) # to write to xlsx file 
library(psych) # useful for descriptive stats 
```

## Data Cleaning 

```{r Data-Cleaning}
# set temporary working directory 
setwd("/Users/gianzlupko/Desktop/Workgroup/dnl_nlp/Studies/empirical_paper")
# load raw data and begin conversion to long data format 
dat <- read_excel("remote_work_empirical_data.xlsx") 

# store a raw data set to for backup 
raw_data <- dat

# view data set 
head(dat) 

```

Six individuals that did not pass attention checks were manually removed before importing the data set. 45 did not complete the survey. Their rows were removed. Regarding T2 retention rate, 65 individuals of the 399 who completed the survey at T1 did not return. Thus, we observed a retention rate of 83.7%. 

```{r}
# remove individuals that did not complete the survey; 45 did not complete the survey 
dat_1 <- dat %>% 
  filter(!Finished == 0) 

# how many individuals (who finished T1) completed the survey at T2? - We see that 65 did not complete T2. 
dat_1 %>% 
  filter(is.na(T2_JobSat_Ave)) 

T2_retention_rate <- (399-65)/399*100
T2_retention_rate

# T2 sample size 
399 - 65

```

## Reshape data from wide to long format 

First I clean column names and assign a unique ID to participants. Then, I break out the full data set into a reasons for the decision and reasons against the decision. I apply topic modeling and sentiment analysis to these data sets individually. Then, after reporting on results from the text analysis, I use the participant's unique ID that I created to bring the NLP features (topic proportions and sentiment scores) back into the original data set to estimate regression equations containing the NLP features with the additional variables in the data set: T2 behavior (DV), burnout (DV), and job satisfaction (covariate). 

By reshaping the data, converting from wide to long format, each reason is first analyzed 

```{r}
# rename columns and store in 
dat_clean <- dat_1 %>%
   rename(Decision = Q4, Reason1 = Q12_1, Reason2 = Q12_2, Reason3 = Q12_3, 
          Reason4 = Q12_4, Reason5 = Q12_5, Reason6 = Q12_6, Reason7 = Q12_7, 
          Reason8 = Q12_8, Reason9 = Q12_9, Reason10 = Q12_10, Against1 = Q33_1, Against2 = Q33_2, Against3 = Q33_3, 
          Against4 = Q33_4, Against5 = Q33_5, Against6 = Q33_6, Against7 = Q33_7, 
          Against8 = Q33_8, Against9 = Q33_9, Against10 = Q33_10, 
          job_sat = T2_JobSat_Ave, behavior_ave = T2_Behavior_3Item_Ave, burnout = T2_Burnout_4Item_Ave)  


# add unique participant id with simple sequence 1:N; this is used to later link transformed data back to individual
dat_clean <- dat_clean %>%
  mutate(participant_id = 1:length(Decision))


# create one data set for reasons for containing reasons data and participant ID 
# used to convert from wide to long format 

reasons_data <- dat_clean %>%
  select(participant_id, Reason1, Reason2, Reason3, Reason3, 
         Reason4, Reason5, Reason6, Reason7, Reason8, Reason9, Reason10)  

# first convert reasons to long data format
reasons_long <- reasons_data %>%
  gather(key = "Reason", 
         value = "Reason_Stated", c(-participant_id)) 


# next create a subset of the original data without the reasons data
# instead, retain only the BRT and decision variable scores to reattach
# to the long data frame

scores_to_merge <- dat_clean %>%
  select(participant_id, behavior_ave, burnout, job_sat)

# now, merge with the long formatted reasons data and merge by participant_id

reasons_formatted <- merge(x = reasons_long, 
                           y = scores_to_merge, 
                           by = "participant_id", 
                           all.y = T)

reasons_formatted 

# finally remove rows with NA values 
reasons_long <- reasons_formatted %>%
  filter(!is.na(Reason_Stated)) 

# add row_id, which will be used after creating topic model
# to re-assign the topic model output back to the unique row id 
reasons_long$row_id <- paste(1:nrow(reasons_long))


# write transformed data set to local to store a copy
#write.xlsx(reasons_long, "reasons_long_formatted.xlsx") 

```


# Text Descriptives

First, of interest to Behavioral Reasoning Theory (BRT) research, I examine the number of reasons for the decision stated by the participants. Next, before calculating additional descriptive statistics, I need to transform the text data into 

### Number of Reasons Stated 
Examining the raw text. 

* 4.7 reasons per participant on average  
    + SD = 2.1 
    + median = 4 
    + min = 1, max = 10 
    
* Median character count is 21 
    + mean = 24.6 
    + SD = 16.32 
    + min = 3, max = 173 
    + 
  
```{r}

# how many reasons provided by participants on average? 

reasons_for_count <- reasons_long %>% 
  group_by(participant_id) %>% 
  count(participant_id) 

mean(reasons_for_count$n) 

# descriptives on reason count data 
psych::describe(reasons_for_count$n)

# word counts and averages 


# calculate average characters for employee responses in Q16 

reasons_char_counts <- reasons_long %>%
  mutate(num = nchar(Reason_Stated)) %>%
   select(num, Reason_Stated)

reasons_char_counts %>% summary() 

describe(reasons_char_counts$num) 




```
# Text Features 

Generate text features like sentiment scores and point-of-speech metrics using the `textfeatures` package in R. I ran this function before pre-processing as these features may benefit from more information captured by stop words and punctuation. 

```{r}
library(textfeatures) 

# run textfeatures workhorse function

reasons_for_features <- textfeatures(reasons_long$Reason_Stated,
                                     sentiment = TRUE, 
                                     word_dims = FALSE, 
                                  normalize = FALSE) 

reasons_long 
# inspect newly generated features 
reasons_for_features <- reasons_for_features %>% 
  select(n_chars, n_uq_chars, n_words, n_uq_words, sent_afinn, sent_bing, 
         sent_syuzhet, sent_vader, n_polite, n_first_person, n_first_personp, n_second_person, n_second_personp, n_third_person) 

reasons_for_features <- reasons_for_features %>% 
  mutate(row_id = seq(nrow(reasons_for_features))) 

# convert reasons long row_id column to numeric to perform data join 
reasons_long$row_id <- as.numeric(reasons_long$row_id) 

# join text features to data with 

reasons_with_features <- reasons_long %>% 
  left_join(reasons_for_features) 

# inspect newly created data set with text features added to reasons data
reasons_with_features 

# save a copy of this data set to local 
#write.xlsx(reasons_with_features, "reasons_for_with_features.xlsx")


```


## Text Variables - Descriptive Stats 

Run descriptive summaries of text variables 

```{r}

# summary stats on text data 
reasons_for_descriptives <- describe(reasons_with_features)

# save text descriptives 
#write.xlsx(reasons_for_descriptives, "reasons_for_descriptives.xlsx") 
  

```


# Text Pre-processing 

The following code uses the `textProcessor()` function in the `stm` package, which


```{r}

# rename 'Reasons_Stated' column to 'document', which the topic model takes
reasons_with_features <- reasons_with_features %>%
  rename(document = Reason_Stated) 

# run data frame through basic pre-processing: remove punctuation, 
# stop words, custom stop words, numbers, and stem.

processed <- textProcessor(documents = reasons_with_features$document, 
                           metadata = reasons_with_features)   

# store processed objects to inspect pre-processing applied 

docs <- processed$documents
vocab <- processed$vocab
meta <- processed

```

## Thresholds 

By default, `prepDocuments` removes words that appear in only one document. Users
can see which documents (e.g. survey responses) had terms removed. If any responses
had no words or the default threshold (e.g. 1), then those are removed as well. Users
can also examine which documents were dropped. 

Below, the `plotRemoved` function allows a researcher to iteratively
tune a threshold parameter to adjust the minimum number of responses that 
a word needs to appear in to be included in subsequent topic modeling.

As an example, requiring that terms appear in at least 5 documents would remove 
4,103 of 5,442 words in the corpus after pre-processing. Depending on the analysts' goal, 
this could be good or bad. An analyst may consider fine tuning the minimum threshold 
parameter in an iterative fashion to achieve desirable results.  

Upper thresholds can also be passed through the `prepDocuments` function 
to remove words that appear in too many responses. 


Currently setting lower threshold to 3, which may be smaller lower threshold for other studies, but is intended to be more inclusive in this study given the short text responses. 
```{r}

# default threshold: 1
out <- prepDocuments(processed$documents, processed$vocab)

# explore minimum threshold 
plotRemoved(processed$documents, lower.thresh = seq(1, 50, by = 1)) 

# set lower threshold of 5, requiring words to appear in at least 5 reasons stated in order to be included in the analysis

out <- prepDocuments(documents = processed$documents,
              vocab = processed$vocab,
              meta = processed$meta, 
              lower.thresh = 3) 


```


# Topic Modeling

## Model Diagnostics 

There are multiple evaluation strategies available. First, researchers can fit topic models across k topics and evaluate model fit and human interpretability based on various criteria developed by researchers. Below  to consider

The function `searchK` allows users to use a data-driven approach to identifying
the number of latent topics. Specifically, the

```{r}

ntopics <- searchK(out$documents, out$vocab, K = c(2,3,4,5,6,7,8,9,10), 
                   data = meta)

# plot the topic diagnostic outputs 
plot(ntopics) 

```



```{r}

stm_select_mod <- selectModel(out$documents, out$vocab, K = 3, max.em.its = 75, data = out$meta, runs = 20, seed = 10210)  

# average semantic coherence score and exclusivity 
# model numerals are shown. Top 4 models are plotted. Models in the top right are best (e.g. high exclusivity and coherence closer to zero)
plotModels(stm_select_mod, pch = c(4,5,6,7),legend.position = "bottomright")

# select Model 1, which is seen as being the most optimized on exclusivity and coherence 
# numbers represent the average for each model and dots represent individual scores 
selectedmodel <- stm_select_mod$runout[[1]]

```






# Topic Modeling


## Text Pre-processing 

Structural Topic Modeling Framework
Resources: 
1. Roberts et al (2018) - STM: R Package for Structural Topic Models: 
https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf

2. University of Oregon: 
https://blogs.uoregon.edu/rclub/2016/04/05/structural-topic-modeling/

```{r}
# rename 'reasons stated' to 'document' as the STM model takes a column titled 'document' as input 
reasons_long <- reasons_long %>%
  rename(document = Reason_Stated) 

# test remove NA from ConfRfor
#reasons_long <- reasons_long %>%
  #filter(!is.na(ConfRfor)) 

# uses textProcessor() function from tm package 
processed <- textProcessor(reasons_long$document, metadata = reasons_long)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)

# run an initial 4-topic fit to explore initial elements in the corpus 
fit0 <- stm(out$documents, # the documents
            out$vocab, # the words
            K = 4, # 10 topics
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral")  

# look at top words
labelTopics(fit0)

plot.STM(fit0, type = "summary") 

# correlations between topics 
round(topicCorr(fit0)$cor, 2) # just the correlations between topics


```


## Evaluate Model Fit 

Model Diagnostics: examining semantic coherence and exclusivity 

```{r, include F}
# identify best k out of 10 topics 
ntopics <- searchK(out$documents, out$vocab, K = c(2, 3 , 4, 5, 6, 7, 8, 9, 10), data = meta)

# fit statistics for k = c(2:10) estimations  
plot(ntopics) 

# selectModel() approach can also be used in addition to searchK() 
# whereby models are selected that optimize on exclusivity and semantic coherence 

stm_select_mod <- selectModel(out$documents, out$vocab, K = 15, 
                              prevalence = ~State, max.em.its = 75, data = out$meta, 
                              runs = 15) 


plotModels(stm_select_mod, pch = c(1,2,3,4), legend.position = "bottomright")
```


## Topics: Reasons for decision to work remote

Reasons For data - Topic Model Comparisons

The following code chunks compare 2, 3, and 4-topic model solutions to select the best one. First, a 2-topic model solution is generated along with corresponding figures, tables, and estimated effect analyses. 
```{r stm-2-topic}

fit_two <- stm(out$documents, # the documents
            out$vocab, # covariates expected to affect topic content 
            prevalence =~ participant_id + State,
            K = 2, # 2 topics
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral") 

sageLabels(fit_two) # only works with content covariates in the model
plot(fit_two)

# correlations between topics 
round(topicCorr(fit0)$cor, 2) # just the correlations between topics
```



Bar chart for 2-topic model solution fit 

```{r}
# Turn the STM object into a data frame. This is necessary so that we can work with it.
td_beta <- tidy(fit_two)

# create re-useable function for subsequent topic model solutions 
# bar chart uses wesanderson color palettes
word_probs <- function(td_beta) { 
  
  td_beta %>%
  # Group by topic
  group_by(topic) %>%
  # Take the top 10 based on beta
  top_n(10, beta) %>%
  # Ungroup
  ungroup() %>%
  # Generate the variables topic and term
  dplyr::mutate(topic = paste0(topic),
                term = reorder_within(term, beta, topic)) %>%
  # And plot it
  ggplot() +
  # Using a bar plot with the terms on the x-axis, the beta on the y-axis, filled by topic
  geom_col(aes(x = term, y = beta, fill = as.factor(topic)),
           alpha = 0.8,
           show.legend = FALSE) +
  # Do a facet_wrap by topic
  facet_wrap(~ topic, scales = "free_y") +
  # And flip the plot
  coord_flip() +
  scale_x_reordered() +
  # Label the x-axis, y-axis, as well as title
  labs(
    y = expression(beta),
    title = "Highest word probabilities for each topic") +
  # And finally define the colors
  scale_fill_manual(values = wes_palette("Darjeeling1"))
  
  
  }

# generate bar chart for 2 topic solution
word_probs(td_beta) 

```



```{r}
# look at highest word probability
labelTopics(fit_two)

plot.STM(fit_two, type = "labels") 

plot.STM(fit_two, type = "summary", main = "2-Topic Expected Proportions") 
# look at 
plot.STM(fit_two, type = "labels", labeltype = c("frex"), main = "2-Topic Solution: Exclusivity")  

# outputs most representative documents for a particular topic
findThoughts(
  # Your topic model
  fit_two,
  texts = reasons_long, 
  n = 3, 
  topics = c(1,2,3)
)
```


Use model outputs to assign categorical variables for regression 

```{r}

# Attitude 
est_two_1 <- estimateEffect(1:2 ~ Att_Ave, stmobj = fit_two, metadata = out$meta, 
                       uncertainty = "None")   

summary(est_two_1) 

# Subjective Norm 

est_two_2 <- estimateEffect(1:2 ~ SN_Ave, stmobj = fit_two, metadata = out$meta, 
                       uncertainty = "None")   

summary(est_two_2)

# Perceived Control

est_two_3 <- estimateEffect(1:2 ~ PC_Ave, stmobj = fit_two, metadata = out$meta, 
                       uncertainty = "None")   

summary(est_two_3)

plot(est_two_1, covariate = "Att_Ave", topics = c(1,2), 
     model = fit_two, 
     method = "continuous") 

# show the estimated relationship between Topic 1 and Attitude 
plot(est_two_1, covariate = "Att_Ave", model = fit_two,
     method = "continuous", xlab = "Attitude", topics = c(1))
```


Generate a 3-topic solution with figures and diagnostics. 

```{r stm-3-topic}

fit_three <- stm(out$documents, # the documents
            out$vocab, # covariates expected to affect topic content 
            prevalence =~ participant_id + State,
            K = 3, # 2 topics
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral", 
            seed = 10901) 

sageLabels(fit_three) # only works with content covariates in the model
plot(fit_three)

# correlations between topics 
round(topicCorr(fit_three)$cor, 2) #correlations between topics

# highest word probabilities bar chart 
td_beta <- tidy(fit_three) 
word_probs(td_beta)

# look at highest word probability
labelTopics(fit_three)

# top terms conditioning on exclusivity of terms by between topics 
plot.STM(fit_three, type = "labels", labeltype = c("frex"), main = "3-Topic Solution: Exclusivity")  

# perpsective plot 
plot.STM(fit_three, type = "perspectives", topics = c(2,3))   

# expected proportion of topics in all reasons for data 
plot.STM(fit_three, type = "summary", main = "3-Topic Expected Proportions") 


# custom approach to summarizing topic proportions 

reasons_for_proportions <- make.dt(fit_three) 

summarize_all(reasons_for_proportions, mean)


# write topic betas to a csv file for manual graphics 

td_beta <- tidy(fit_three) 
td_beta <- td_beta %>%
   mutate(topic = ifelse(topic == 1, "To Protect Others",  
                         ifelse(topic == 2, "To Protect Self", 
                                ifelse(topic == 3, "To Avoid Spread", NA)))) %>%
  mutate(topic = as.factor(topic)) %>%
  group_by(topic) %>% arrange(desc(beta)) 


# update select character strings to undo stemming for publication 

td_beta$term <- str_replace_all(td_beta$term, "healthi", "health")
td_beta$term <- str_replace_all(td_beta$term, "safeti", "safety")
td_beta$term <- str_replace_all(td_beta$term, "famili", "family")
td_beta$term <- str_replace_all(td_beta$term, "peopl", "people")
td_beta$term <- str_replace_all(td_beta$term, "exampl", "example")
td_beta$term <- str_replace_all(td_beta$term, "practic", "practices")
word_probs(td_beta) 

```
Calculate estimated effects for the 3-topic solution

```{r stm-3-topic-estimates}

# Attitude 
est_three_1 <- estimateEffect(1:3 ~ Att_Ave, stmobj = fit_three, metadata = out$meta, 
                              uncertainty = "None")    

summary(est_three_1) 

# Subjective Norm 

est_three_2 <- estimateEffect(1:3 ~ SN_Ave, stmobj = fit_three, metadata = out$meta, 
                       uncertainty = "None")   

summary(est_three_2)

# Perceived Control

est_three_3 <- estimateEffect(1:3 ~ PC_Ave, stmobj = fit_three, metadata = out$meta, 
                       uncertainty = "None")   

summary(est_three_3)



plot(est_two_1, covariate = "Att_Ave", topics = c(1,2), 
     model = fit_two, 
     method = "continuous") 

# show the estimated relationship between Topic 3 and PC 
plot(est_three_3, covariate = "PC_Ave", model = fit_three,
     method = "continuous", xlab = "Perceived Control", topics = c(3))


```


Generate a 4-topic model solution for comparison


```{r stm-4-topic-model}

fit_four <- stm(out$documents, # the documents
            out$vocab, # covariates expected to affect topic content 
            prevalence =~ participant_id + State,
            K = 4, # 2 topics
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral") 

sageLabels(fit_four) # only works with content covariates in the model
plot(fit_four)

# correlations between topics 
round(topicCorr(fit_four)$cor, 2) # shows that correlations


# highest word probabilities bar chart 
td_beta <- tidy(fit_four) 
word_probs(td_beta)

# look at highest word probability
labelTopics(fit_four)

# top terms conditioning on exclusivity of terms by between topics 
plot.STM(fit_four, type = "labels", labeltype = c("frex"), main = "4-Topic Solution: Exclusivity")  

# perpsective plot: can add additional context on differentiation b/w two specific topics in the solution
plot.STM(fit_four, type = "perspectives", topics = c(3,4))   

# expected proportion of topics in all reasons for data 
plot.STM(fit_four, type = "summary", main = "4-Topic Expected Proportion")  

```



Calculate estimated effects for 4-topic solution 

```{r stm-4-topic-estimates}

# Attitude 
est_four_1 <- estimateEffect(1:4 ~ Att_Ave, stmobj = fit_four, metadata = out$meta, 
                             uncertainty = "None")   

summary(est_four_1) 

# Subjective Norm 

est_four_2 <- estimateEffect(1:4 ~ SN_Ave, stmobj = fit_four, metadata = out$meta, 
                       uncertainty = "None")   
summary(est_four_2)

# Perceived Control

est_four_3 <- estimateEffect(1:4 ~ PC_Ave, stmobj = fit_four, metadata = out$meta, 
                       uncertainty = "None")   
summary(est_four_3)

```




```{r stm-fit-statistics}
set.seed(6110) 
# use searchK() function to calculate fit statistics for 2, 3, and 4 topic solutions 
ntopics <- searchK(out$documents, out$vocab, K = c(2, 3, 4), data = meta)

# grab fit statistic values with the results df 
ntopics$results

# plot ntopics for fit statistic plots 
plot(ntopics) 

```



## Topics: Reasons Against decision

Reasons Against Data

Reasons Against data - Topic Model Comparisons

The following code chunks compare 2, 3, and 4-topic model solutions to select the best model for the reasons against data. First, a 2-topic model solution is generated along with corresponding figures, tables, and estimated effect analyses. 


```{r Against-Data-Cleaning}
# rename columns

against <- dat %>%
   rename(Against1 = Q33_1, Against2 = Q33_2, Against3 = Q33_3, 
          Against4 = Q33_4, Against5 = Q33_5, Against6 = Q33_6, Against7 = Q33_7, 
          Against8 = Q33_8, Against9 = Q33_9, Against10 = Q33_10)  


# first add unique participant id 
against1 <- against %>%
  mutate(participant_id = 1:length(Decision))

# use duplicated() to check that each participant_id is unique 
duplicated(against1[ ,c("participant_id")])

# after confirming that all ids are unique
# create one data set of reasons that will be 
# used to convert from wide to long format 

against_df <- against1 %>%
  select(participant_id, Against1, Against2, Against3, Against4, 
         Against5, Against6, Against7, Against8, Against9, Against10)  

# first convert reasons to long data format
against_long <- against_df %>%
  gather(key = "Against", 
         value = "Against_Stated", c(-participant_id)) 


# next create a subset of the original data without the reasons data
# instead, retain only the BRT and decision variable scores to reattach
# to the long data frame

against_to_merge <- against1 %>%
  select(participant_id, ConfRfor, ConfRag, Att_Ave, SN_Ave, PC_Ave, MacroReasoning, 
         SN_general_Ave, SN_work_Ave, ProReas, ConReas, State)

# now, merge with the long formatted reasons data and merge by participant_id

against_formatted <- merge(x = against_long, 
                           y = against_to_merge, 
                           by = "participant_id", 
                           all.y = T)


# finally remove rows with NA values 

against_long <- against_formatted %>%
  filter(!is.na(Against_Stated)) 


# add row_id, which will be used after creating topic model
# to re-assign the topic model output back to the unique row id 
against_long$row_id <- paste(1:nrow(against_long))
```


STM processing and model diagnostics 
```{r}

# renmae Against_Stated to 'document' for tm processing 
against_long <- against_long %>%
  rename(document = Against_Stated) 

# uses textProcessor() function from tm package 
against_processed <- textProcessor(against_long$document, metadata = against_long)

against_out <- prepDocuments(against_processed$documents, 
                             against_processed$vocab, 
                             against_processed$meta)


# two methods for identifying model fit 

# identify best k = c(2:10)
against_ntopics <- searchK(against_out$documents, against_out$vocab, K = c(2, 3, 4, 5, 6, 7, 8, 9, 10), data = meta)
plot(against_ntopics)

# show comparison of fit statistics for k = c(2:4)
against_ntopics <- searchK(against_out$documents, against_out$vocab, K = c(2, 3, 4), data = meta)
# plot fit statistic values for solutions on reasons against data 
plot(against_ntopics)

# grab fit statistics for topic model solutions on reasons against data 
against_ntopics$results

```


2-topic model solution for reasons against data 

```{r against-2-topic-solution}

against_fit_two <- stm(against_out$documents, # the documents
            against_out$vocab, # the words
            K = 2, # 3 topics
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = against_out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral")  



# highest word probabilities bar chart 
td_beta <- tidy(against_fit_two) 
word_probs(td_beta)

# look at highest word probability
labelTopics(against_fit_two)

# top terms conditioning on exclusivity of terms by between topics 
plot.STM(against_fit_two, type = "labels", labeltype = c("frex"), main = "2-Topic Solution: Exclusivity")  

# perpsective plot: can add additional context on differentiation b/w two specific topics in the solution
plot.STM(against_fit_two, type = "perspectives", topics = c(1,2))   

# expected proportion of topics in all reasons for data 
plot.STM(against_fit_two, type = "summary", main = "2-Topic Expected Proportion")  

# look at top words by category
labelTopics(against_fit_two)

```



2-topic solution, Reasons Against - Estimated Effects 
```{r}

# Attitude 
against_est_two_1 <- estimateEffect(1:2 ~ Att_Ave, stmobj = against_fit_two, metadata = against_out$meta, 
                                    uncertainty = "None")    

summary(against_est_two_1) 

# Subjective Norm 

against_est_two_2 <- estimateEffect(1:2 ~ SN_Ave, stmobj = against_fit_two, metadata = against_out$meta, 
                       uncertainty = "None")   
summary(against_est_two_2)

# Perceived Control

against_est_two_3 <- estimateEffect(1:2 ~ PC_Ave, stmobj = against_fit_two, metadata = against_out$meta, 
                       uncertainty = "None")   
summary(against_est_two_3)
```



3-topic model solution for reasons against data 

```{r}

against_fit_three <- stm(against_out$documents, # the documents
            against_out$vocab, # the words
            K = 3, # 3 topics
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = against_out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral")  



# highest word probabilities bar chart 
td_beta <- tidy(against_fit_three) 
word_probs(td_beta)

# look at highest word probability
labelTopics(against_fit_three)

# top terms conditioning on exclusivity of terms by between topics 
plot.STM(against_fit_three, type = "labels", labeltype = c("frex"), main = "3-Topic Solution: Exclusivity")  

# perpsective plot: can add additional context on differentiation b/w two specific topics in the solution
plot.STM(against_fit_three, type = "perspectives", topics = c(1,2))   

# expected proportion of topics in all reasons for data 
plot.STM(against_fit_three, type = "summary", main = "3-Topic Expected Proportion")  

# look at top words by category
labelTopics(against_fit_three)

# custom proption of topic in overall corpus for 3-topic model 

reasons_against_prop <- make.dt(against_fit_three)

summarize_all(reasons_against_prop, mean) 

td_beta

#write.csv(td_beta, "against_fit_k3_beta.csv") 

```


Reasons Against Estimated Effects 3-topic solution 

```{r}

# Attitude 
set.seed(1061) 
against_est_three_1 <- estimateEffect(1:3 ~ Att_Ave, stmobj = against_fit_three, metadata = against_out$meta, 
                                    uncertainty = "None")    

summary(against_est_three_1) 

# Subjective Norm 

against_est_three_2 <- estimateEffect(1:3 ~ SN_Ave, stmobj = against_fit_three, metadata = against_out$meta, 
                       uncertainty = "None")   
summary(against_est_three_2)

# Perceived Control

against_est_three_3 <- estimateEffect(1:3 ~ PC_Ave, stmobj = against_fit_three, metadata = against_out$meta, 
                       uncertainty = "None")   
summary(against_est_three_3)


# show the estimated relationship between Against Topic 3 and PC 
plot(against_est_three_1, covariate = "Att_Ave", model = against_fit_three,
     method = "continuous", xlab = "Attitude", topics = c(1, 2), 
     main = "Attitude by Topic 1 and Topic 2 \n in Reasons Against")  


```


4-topic model solution for reasons against data 


```{r stm-against-4-topic-model}

against_fit_four <- stm(against_out$documents, # the documents
            against_out$vocab, # the words
            K = 4, 
            max.em.its = 75, # set to run for a maximum of 75 EM iterations
            data = against_out$meta, # all the variables (we're not actually including any predictors in this model, though)
            init.type = "Spectral")  



# highest word probabilities bar chart 
td_beta <- tidy(against_fit_four) 
word_probs(td_beta)

# look at highest word probability
labelTopics(against_fit_four)

# top terms conditioning on exclusivity of terms by between topics 
plot.STM(against_fit_four, type = "labels", labeltype = c("frex"), main = "4-Topic Solution: Exclusivity")  

# perpsective plot: can add additional context on differentiation b/w two specific topics in the solution
plot.STM(against_fit_four, type = "perspectives", topics = c(1,2))   

# expected proportion of topics in all reasons for data 
plot.STM(against_fit_four, type = "summary", main = "4-Topic Expected Proportion")  

# look at top words by category
labelTopics(against_fit_four)
```


Reasons Against Estimated Effects 3-topic solution 

```{r 4-topic-estimated-effects}

# Attitude 
set.seed(10826) 
against_est_four_1 <- estimateEffect(1:4 ~ Att_Ave, stmobj = against_fit_four, metadata = against_out$meta, 
                                    uncertainty = "None")    

summary(against_est_four_1) 

# Subjective Norm 

against_est_four_2 <- estimateEffect(1:4 ~ SN_Ave, stmobj = against_fit_four, metadata = against_out$meta, 
                       uncertainty = "None")   
summary(against_est_four_2)

# Perceived Control

against_est_four_3 <- estimateEffect(1:4 ~ PC_Ave, stmobj = against_fit_four, metadata = against_out$meta, 
                       uncertainty = "None")   
summary(against_est_four_3)


# show the estimated relationship between Against Topic 3 and PC 
plot(against_est_four_1, covariate = "Att_Ave", model = against_fit_four,
     method = "continuous", xlab = "Attitude", topics = c(3), 
     main = "Attitude by Topic 3 \n in Reasons Against")  


```



# Descriptive Statistics 



## Counts and summaries 

Median character count, total words, total unique words ... 




# N-Grams 


## Most common n-grams (full corpus) 




## N-grams by focal groups

The following code chunk visualizes different sets of top word co-occurances by groups: 





```{r}
# bigrams for ___ 
occupation_bigrams %>%
  filter(occupation_group == "group1") %>% # to update filter by
  count(bigram) %>% arrange(desc(n)) %>% head(5) %>%
  ggplot(aes(x = reorder(bigram, n), y = n, fill = bigram)) + geom_bar(stat = "identity") + 
  coord_flip() + theme(legend.position = "none") + 
  xlab("Bigram") + ylab("Freq.") 



```



# Sentiment Analysis 

Classify response sentiment using the pre-trained Vader dictionary in Python. Then, return to R for subsequent analysis. 

```{python}
# store df in python from r 
# to update with new data  

df_complete = r.df_complete

# libaries 
import numpy as np
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import WordPunctTokenizer
from nltk.stem.wordnet import WordNetLemmatizer
import matplotlib.pyplot as plt


from nltk.sentiment.vader import SentimentIntensityAnalyzer
# Creating instance
analyzer = SentimentIntensityAnalyzer()
april_sent = april_complete.copy(deep = True)
april_sent['Positive'] = [analyzer.polarity_scores(x)['pos'] for x in april_sent['Q16_workload']]
april_sent['Negative'] = [analyzer.polarity_scores(x)['neg'] for x in april_sent['Q16_workload']]
april_sent['Neutral'] = [analyzer.polarity_scores(x)['neu'] for x in april_sent['Q16_workload']]
april_sent['Compound'] = [analyzer.polarity_scores(x)['compound'] for x in april_sent['Q16_workload']]
april_sent['Positive'] = april_sent['Positive'].round(2)*100
april_sent['Negative'] = april_sent['Negative'].round(2)*100
april_sent['Neutral'] = april_sent['Neutral'].round(2)*100
april_sent['Compound'] = april_sent['Compound'].round(2)*100
april_sent.head()
```

Following classification in Python, import back into R. 

```{r}

# import df from python
# to update with new data

df_sentiment <- py$df_sent

april_sentiment %>%
  select(mseo_group, supervisory, center, Positive, Negative, 
         Neutral, Compound) 

lm(Positive ~  group_1 + supervisory, 
   data = april_sentiment)  %>% summary() 

april_sentiment %>%
  ggplot(aes(x = Negative, y = Positive, 
             color = group_1)) + geom_point() + 
  geom_smooth(method = "lm") + xlim(0,50) + ylim(0,60) 
  
```


